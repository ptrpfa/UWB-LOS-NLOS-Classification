{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading of Datasets\n",
    "\n",
    "Based on the optimised parameters that we have derived from the data mining process, we will determine the training and testing times of each classifier we have chosen.\n",
    "\n",
    "The datasets that we have chosen are:\n",
    "- `noncir_ss_scaled_trimmed_cir_pca_ss_scaled.pkl`: Non-Cir (Standard Scaled after feature selection) with CIR Statistical Measures (PCA and Standard Scaled)\n",
    "- `noncir_ss_scaled_trimmed_cir_ss_scaled.pkl`: Non-CIR (Standard Scaled after feature selection) with CIR Statistical Measures (Standard Scaled)\n",
    "- `noncir_ss_scaled_trimmed_cir_pca.pkl`: Non-CIR (Standard Scaled after feature selection) with CIR Statistical Measures (PCA)\n",
    "\n",
    "The datasets will be loaded into `dataset_1`, `dataset_2`, `dataset_3` respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "\n",
    "dataset_1 = load_from_pickle(\"noncir_ss_scaled_trimmed_cir_pca_ss_scaled.pkl\")\n",
    "dataset_2 = load_from_pickle(\"noncir_ss_scaled_trimmed_cir_ss_scaled.pkl\")\n",
    "dataset_3 = load_from_pickle(\"noncir_ss_scaled_trimmed_cir_pca.pkl\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machine (SVM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset_1 (noncir_ss_scaled_trimmed_cir_pca_ss_scaled)\n",
    "\n",
    "Evaluation of the training and testing time of `dataset_1`. With a 70:30 training and test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Select the features to be used for Support Vector Classification\n",
    "X = dataset_1.drop(columns = 'NLOS')\n",
    "Y = dataset_1[['NLOS']].to_numpy()\n",
    "Y = Y.reshape(-1)\n",
    "\n",
    "# Split dataset into 70% training and 30% test\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size = TEST_SIZE, random_state = RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Time:  6.014267921447754\n"
     ]
    }
   ],
   "source": [
    "clf = SVC(kernel = 'linear', C = 0.01, random_state = RANDOM_STATE)\n",
    "t0 = time.time()\n",
    "clf.fit(x_train, y_train)\n",
    "print(\"Training Time: \", time.time() - t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Time:  1.4445459842681885\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "clf.predict(x_test)\n",
    "print(\"Testing Time: \", time.time() - t0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset_2 (noncir_ss_scaled_trimmed_cir_ss_scaled)\n",
    "\n",
    "Evaluation of the training and testing time of `dataset_2`. With a 70:30 training and test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the features to be used for Support Vector Classification\n",
    "X = dataset_2.drop(columns = 'NLOS')\n",
    "Y = dataset_2[['NLOS']].to_numpy()\n",
    "Y = Y.reshape(-1)\n",
    "\n",
    "# Split dataset into 70% training and 30% test\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size = TEST_SIZE, random_state = RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Time:  144.48278498649597\n"
     ]
    }
   ],
   "source": [
    "# Create a SVM Classifier with the kernel of linear for linear hyperplane\n",
    "clf = SVC(kernel = 'linear', C = 0.001, random_state = RANDOM_STATE)\n",
    "t0 = time.time()\n",
    "clf.fit(x_train, y_train)\n",
    "print(\"Training Time: \", time.time() - t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Time:  19.74851965904236\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "clf.predict(x_test)\n",
    "print(\"Testing Time: \", time.time() - t0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Radial Basis Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset_1 (noncir_ss_scaled_trimmed_cir_pca_ss_scaled)\n",
    "\n",
    "Evaluation of the training and testing time of `dataset_1`. With a 70:30 training and test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the features to be used for Support Vector Classification\n",
    "X = dataset_1.drop(columns = 'NLOS')\n",
    "Y = dataset_1[['NLOS']].to_numpy()\n",
    "Y = Y.reshape(-1)\n",
    "\n",
    "# Split dataset into 70% training and 30% test\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size = TEST_SIZE, random_state = RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Time:  9.284297704696655\n"
     ]
    }
   ],
   "source": [
    "# Create a SVM Classifier with the kernel of linear for linear hyperplane\n",
    "clf = SVC(kernel = 'rbf', C = 0.1, gamma = 'auto', random_state = RANDOM_STATE)\n",
    "t0 = time.time()\n",
    "clf.fit(x_train, y_train)\n",
    "print(\"Training Time: \", time.time() - t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Time:  6.838964223861694\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "clf.predict(x_test)\n",
    "print(\"Testing Time: \", time.time() - t0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset_2 (noncir_ss_scaled_trimmed_cir_ss_scaled)\n",
    "\n",
    "Evaluation of the training and testing time of `dataset_2`. With a 70:30 training and test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the features to be used for Support Vector Classification\n",
    "X = dataset_2.drop(columns = 'NLOS')\n",
    "Y = dataset_2[['NLOS']].to_numpy()\n",
    "Y = Y.reshape(-1)\n",
    "\n",
    "# Split dataset into 70% training and 30% test\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size = TEST_SIZE, random_state = RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Time:  186.82493042945862\n"
     ]
    }
   ],
   "source": [
    "# Create a SVM Classifier with the kernel of linear for linear hyperplane\n",
    "clf = SVC(kernel = 'rbf', C = 0.1, gamma = 'auto', random_state = RANDOM_STATE)\n",
    "t0 = time.time()\n",
    "clf.fit(x_train, y_train)\n",
    "print(\"Training Time: \", time.time() - t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Time:  83.8489122390747\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "clf.predict(x_test)\n",
    "print(\"Testing Time: \", time.time() - t0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset_3 (noncir_ss_scaled_trimmed_cir_pca)\n",
    "\n",
    "Evaluation of the training and testing time of `dataset_3`. With a 70:30 training and test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the features to be used for Support Vector Classification\n",
    "X = dataset_3.drop(columns = 'NLOS')\n",
    "Y = dataset_3[['NLOS']].to_numpy()\n",
    "Y = Y.reshape(-1)\n",
    "\n",
    "# Split dataset into 70% training and 30% test\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size = TEST_SIZE, random_state = RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Time:  42.361613750457764\n"
     ]
    }
   ],
   "source": [
    "clf = SVC(kernel = 'rbf', C = 0.0001, gamma = 'auto', random_state = RANDOM_STATE)\n",
    "t0 = time.time()\n",
    "clf.fit(x_train, y_train)\n",
    "print(\"Training Time: \", time.time() - t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Time:  28.251997470855713\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "clf.predict(x_test)\n",
    "print(\"Testing Time: \", time.time() - t0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset_1 (noncir_ss_scaled_trimmed_cir_pca_ss_scaled)\n",
    "\n",
    "Evaluation of the training and testing time of `dataset_1`. With a 70:30 training and test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the features to be used for Support Vector Classification\n",
    "X = dataset_1.drop(columns = 'NLOS')\n",
    "Y = dataset_1[['NLOS']].to_numpy()\n",
    "Y = Y.reshape(-1)\n",
    "\n",
    "# Split dataset into 70% training and 30% test\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size = TEST_SIZE, random_state = RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Time:  5.480480670928955\n"
     ]
    }
   ],
   "source": [
    "# Create a SVM Classifier with the kernel of linear for linear hyperplane\n",
    "clf = MLPClassifier(hidden_layer_sizes = (5, 5, 5), activation = 'relu', learning_rate = 'constant', solver = 'adam', max_iter = 1000, random_state = RANDOM_STATE)\n",
    "t0 = time.time()\n",
    "clf.fit(x_train, y_train)\n",
    "print(\"Training Time: \", time.time() - t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Time:  0.0042514801025390625\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "clf.predict(x_test)\n",
    "print(\"Testing Time: \", time.time() - t0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset_2 (noncir_ss_scaled_trimmed_cir_ss_scaled)\n",
    "\n",
    "Evaluation of the training and testing time of `dataset_2`. With a 70:30 training and test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the features to be used for Support Vector Classification\n",
    "X = dataset_2.drop(columns = 'NLOS')\n",
    "Y = dataset_2[['NLOS']].to_numpy()\n",
    "Y = Y.reshape(-1)\n",
    "\n",
    "# Split dataset into 70% training and 30% test\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size = TEST_SIZE, random_state = RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Time:  36.62067103385925\n"
     ]
    }
   ],
   "source": [
    "clf = MLPClassifier(hidden_layer_sizes = (2, 2), activation = 'relu', learning_rate = 'constant', solver = 'adam', max_iter = 1000, random_state = RANDOM_STATE)\n",
    "t0 = time.time()\n",
    "clf.fit(x_train, y_train)\n",
    "print(\"Training Time: \", time.time() - t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Time:  0.03354620933532715\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "clf.predict(x_test)\n",
    "print(\"Testing Time: \", time.time() - t0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset_3 (noncir_ss_scaled_trimmed_cir_pca)\n",
    "\n",
    "Evaluation of the training and testing time of `dataset_3`. With a 70:30 training and test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Time:  6.067868947982788\n"
     ]
    }
   ],
   "source": [
    "clf = MLPClassifier(hidden_layer_sizes = (10, 10, 10), activation = 'relu', learning_rate = 'constant', solver = 'adam', max_iter = 1000, random_state = RANDOM_STATE)\n",
    "t0 = time.time()\n",
    "clf.fit(x_train, y_train)\n",
    "print(\"Training Time: \", time.time() - t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Time:  0.003998517990112305\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "clf.predict(x_test)\n",
    "print(\"Testing Time: \", time.time() - t0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "PURITY_MEASURE = ['entropy', 'gini']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset_1 (noncir_ss_scaled_trimmed_cir_pca_ss_scaled)\n",
    "\n",
    "Evaluation of the training and testing time of `dataset_1`. With a 70:30 training and test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the features to be used for Support Vector Classification\n",
    "X = dataset_1.drop(columns = 'NLOS')\n",
    "Y = dataset_1[['NLOS']].to_numpy()\n",
    "Y = Y.reshape(-1)\n",
    "\n",
    "# Split dataset into 70% training and 30% test\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size = TEST_SIZE, random_state = RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entropy Training Time:  13.483222961425781\n",
      "Gini Training Time:  10.056505680084229\n"
     ]
    }
   ],
   "source": [
    "clf = RandomForestClassifier(max_depth = 10, criterion = PURITY_MEASURE[0], random_state = RANDOM_STATE)\n",
    "t0 = time.time()\n",
    "clf.fit(x_train, y_train)\n",
    "print(\"Entropy Training Time: \", time.time() - t0)\n",
    "\n",
    "\n",
    "clf2 = RandomForestClassifier(max_depth = 10, criterion = PURITY_MEASURE[1], random_state = RANDOM_STATE)\n",
    "t0 = time.time()\n",
    "clf2.fit(x_train, y_train)\n",
    "print(\"Gini Training Time: \", time.time() - t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entropy Testing Time:  0.054071903228759766\n",
      "Gini Testing Time:  0.05740928649902344\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "clf.predict(x_test)\n",
    "print(\"Entropy Testing Time: \", time.time() - t0)\n",
    "\n",
    "t0 = time.time()\n",
    "clf2.predict(x_test)\n",
    "print(\"Gini Testing Time: \", time.time() - t0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset_2 (noncir_ss_scaled_trimmed_cir_ss_scaled)\n",
    "\n",
    "Evaluation of the training and testing time of `dataset_2`. With a 70:30 training and test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the features to be used for Support Vector Classification\n",
    "X = dataset_2.drop(columns = 'NLOS')\n",
    "Y = dataset_2[['NLOS']].to_numpy()\n",
    "Y = Y.reshape(-1)\n",
    "\n",
    "# Split dataset into 70% training and 30% test\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size = TEST_SIZE, random_state = RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entropy Training Time:  34.38670086860657\n",
      "Gini Training Time:  31.081051349639893\n"
     ]
    }
   ],
   "source": [
    "clf = RandomForestClassifier(max_depth = 10, criterion = PURITY_MEASURE[0], random_state = RANDOM_STATE)\n",
    "t0 = time.time()\n",
    "clf.fit(x_train, y_train)\n",
    "print(\"Entropy Training Time: \", time.time() - t0)\n",
    "\n",
    "\n",
    "\n",
    "clf2 = RandomForestClassifier(max_depth = 10, criterion = PURITY_MEASURE[1], random_state = RANDOM_STATE)\n",
    "t0 = time.time()\n",
    "clf2.fit(x_train, y_train)\n",
    "print(\"Gini Training Time: \", time.time() - t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entropy Testing Time:  0.13614916801452637\n",
      "Gini Testing Time:  0.13197803497314453\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "clf.predict(x_test)\n",
    "print(\"Entropy Testing Time: \", time.time() - t0)\n",
    "\n",
    "t0 = time.time()\n",
    "clf2.predict(x_test)\n",
    "print(\"Gini Testing Time: \", time.time() - t0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset_3 (noncir_ss_scaled_trimmed_cir_pca)\n",
    "\n",
    "Evaluation of the training and testing time of `dataset_3`. With a 70:30 training and test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the features to be used for Support Vector Classification\n",
    "X = dataset_3.drop(columns = 'NLOS')\n",
    "Y = dataset_3[['NLOS']].to_numpy()\n",
    "Y = Y.reshape(-1)\n",
    "\n",
    "# Split dataset into 70% training and 30% test\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size = TEST_SIZE, random_state = RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entropy Training Time:  13.614278316497803\n",
      "Gini Training Time:  10.226720094680786\n"
     ]
    }
   ],
   "source": [
    "clf = RandomForestClassifier(max_depth = 10, criterion = PURITY_MEASURE[0], random_state = RANDOM_STATE)\n",
    "t0 = time.time()\n",
    "clf.fit(x_train, y_train)\n",
    "print(\"Entropy Training Time: \", time.time() - t0)\n",
    "\n",
    "\n",
    "\n",
    "clf2 = RandomForestClassifier(max_depth = 10, criterion = PURITY_MEASURE[1], random_state = RANDOM_STATE)\n",
    "t0 = time.time()\n",
    "clf2.fit(x_train, y_train)\n",
    "print(\"Gini Training Time: \", time.time() - t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entropy Testing Time:  0.05671858787536621\n",
      "Gini Testing Time:  0.0545496940612793\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "clf.predict(x_test)\n",
    "print(\"Entropy Testing Time: \", time.time() - t0)\n",
    "\n",
    "t0 = time.time()\n",
    "clf2.predict(x_test)\n",
    "print(\"Gini Testing Time: \", time.time() - t0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
